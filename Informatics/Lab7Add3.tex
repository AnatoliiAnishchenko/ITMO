\documentclass[10pt]{beamer}

\usepackage[english, russian]{babel}
\usepackage{setspace, amsmath}

\begin{document}
	\defbeamertemplate*{frametitle}{shadow theme}{
		\vskip 2pt \leavevmode
		\hbox to \paperwidth {
			\hbox to 2.5cm {
				\hskip -28pt \vbox to -2pt {
					\vskip-14pt \includegraphics[width=2.5cm] {logo.png}
				}
			}
			\begin{beamercolorbox}[wd=\paperwidth,ht=2ex,dp=3pt,left]{title in head/foot}
				\insertframetitle
			\end{beamercolorbox}
		}
	}
	
	\begin{frame}
		\frametitle{Определение термина <<информатика>>}
		Информатика – дисциплина, изучающая свойства и структуру информации, закономерности ее создания, преобразования, накопления, передачи и использования.

		Англ: informatics $=$ information technology $+$ computer science $+$ information theory

		\begin{center}
			\textbf{Важные даты}
		\end{center}
		\begin{itemize}
			\item 1956 – появление термина «информатика» (нем. Informatik, Штейнбух)
			\item 1968 – первое упоминание в СССР (информология, Харкевич)
			\item 197Х – информатика стала отдельной наукой
			\item 4 декабря – день российской информатики
		\end{itemize}
	\end{frame}
  
	\begin{frame}
		\frametitle{Терминология: информация и данные}
		Международный стандарт ISO/IEC 2382:2015 \\
		<<Information technology – Vocabulary>> (вольный пересказ):\\
		\hskip 20pt Информация – знания относительно фактов, событий, вещей, идей и понятий. \\
		\hskip 20pt Данные – форма представления информации в виде, пригодном для передачи или обработки.

		\begin{itemize}
			\item Что есть предмет информатики: информация или данные?
			\item Как измерить информацию? Как измерить данные? \\
				Пример: <<Байкал — самое глубокое озеро Земли>>.
		\end{itemize}
    	\end{frame}
  
	\begin{frame}
		\frametitle{Измерение количества информации}
		Количество информации $\equiv$ информационная энтропия – это численная мера непредсказуемости информации. Количество информации в некотором объекте определяется непредсказуемостью состояния, в котором находится этот объект.
Пусть $i(s)$ — функция для измерения количеств информации в объекте $s$, состоящем из $n$ независимых частей $s_k$, где $k$ изменяется от $1$ до $n$. Тогда свойства меры количества информации $i(s)$ таковы:

		\begin{itemize}
			\item Неотрицательность: $i(s) \geq 0$.
			\item Принцип предопределённости: если об объекте уже все известно, то $i(s) = 0$.
			\item Аддитивность: $i(s) = \sum{i(s_k)}$ по всем $k$.
			\item Монотонность: $i(s)$ монотонна при монотонном изменении вероятностей.
		\end{itemize}
    	\end{frame}
  
	\begin{frame}
		\frametitle{Пример применения меры Хартли на практике}
		\begin{small}
		\textbf{Пример 1.} 
		Ведущий загадывает число от $1$ до $64$. Какое количество вопросов типа <<да-нет>> понадобится, чтобы гарантировано угадать число?
		\begin{itemize}
			\item \underline{Первый} вопрос: <<Загаданное число меньше 32?>>. Ответ: <<Да>>.
			\item \underline{Второй} вопрос: <<Загаданное число меньше 16?>>. Ответ: <<Нет>>.
			\item ...
			\item \underline{Шестой} вопрос (в худшем случае) точно приведёт к верному ответу.
			\item Значит, в соответствии с мерой Хартли в загадке ведущего содержится ровно $\log_2 64 = 6$ бит непредсказуемости (т. е. информации).
		\end{itemize}
		
		\textbf{Пример 2.}
		 Ведущий держит за спиной ферзя и собирается поставить его на произвольную клетку доски. Насколько непредсказуемо его решение?
		\begin{itemize}
			\item Всего на доске $8 \times 8$ клеток, а цвет ферзя может быть белым или чёрным, т. е. всего возможно $8 \times 8 \times 2 = 128$ равновероятных состояний.
			\item Значит, количество информации по Хартли равно $\log_2 128 = 7$ бит.
		\end{itemize}
		\end{small}
    	\end{frame}
  
	\begin{frame}
		\frametitle{Анализ свойств меры Хартли}
		Экспериментатор одновременно подбрасывает монету (М) и кидает игральную кость (К).\\
		Какое количество информации содержится в эксперименте (Э)?
		\vskip 0.5cm
		\textbf{Аддитивность}:\\
		$i($Э$) = i($М$) + i($К$) \Rightarrow i(12 исходов) = i(2 исхода) + i(6 исходов): \log_x 12 = \log_x 2 + \log_x 6$\\
		\vskip 0.3cm
		\textbf{Неотрицательность}:\\
		Функция $\log_x N$ неотрицательно при любом $x > 1$ и $N \geq 1$.
		\vskip 0.3cm
		\textbf{Монотонность}:\\
		С увеличением $p($М$)$ или $p($К$)$ функция $i($Э$)$ монотонно возрастает.\\
		\vskip 0.3cm
		\textbf{Принцип предопределённости}:\\
		При наличии всегда только одного исхода (монета и кость с магнитом) количество информации равно нулю: $\log_x 1 + \log_x 1 = 0$.
    	\end{frame}
\end{document}